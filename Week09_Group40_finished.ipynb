{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw9_Jan.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lvApF_zUmZg1"},"source":["# Week 09 Group 40"]},{"cell_type":"code","metadata":{"id":"IvP-5ZcUDtAB"},"source":["import numpy as np\r\n","import tensorflow as tf\r\n","import tensorflow_datasets as tfds\r\n","import matplotlib.pyplot as plt\r\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btkB83RfEGua"},"source":["##Generate our Dataset and Preprocess\r\n"]},{"cell_type":"code","metadata":{"id":"SljNjnclqgIV"},"source":["class gen_class(object):\r\n","\r\n","\r\n","    def __init__(self, n ):\r\n","        self.seq_length = n\r\n","        self.dataset_length = 0\r\n","        self.sequence_to_choose_from = range(1,10)\r\n","        self.seq = []\r\n","\r\n","\r\n","    def __iter__(self):\r\n","        return self\r\n","\r\n","\r\n","    def __next__(self):\r\n","        return self.next()\r\n","\r\n","    def next(self):\r\n","        if self.dataset_length < 64*64: \r\n","            self.dataset_length +=1\r\n","            q1 = random.choice(self.sequence_to_choose_from)\r\n","            q2 = random.choice(self.sequence_to_choose_from)\r\n","            # sample until the queried digits arent the same\r\n","            while (q1 == q2):\r\n","              q2 = random.choice(self.sequence_to_choose_from)\r\n","            self.seq = []\r\n","            for _ in range(self.seq_length):\r\n","              self.seq.append([q1,q2,random.choice(self.sequence_to_choose_from)])\r\n","            #append relevant queries\r\n","            \r\n","            target1 = self.seq.count([q1,q2,q1])\r\n","            target2 = self.seq.count([q1,q2,q2])\r\n","            if(target1 < target2):\r\n","                target = 1\r\n","            elif(target2 < target1):\r\n","                target = 2\r\n","            elif(target1 == target2):\r\n","                target = 0\r\n","            target = tf.one_hot(target,3)\r\n","            return  (tf.reshape(tf.convert_to_tensor(self.seq), [25,3]), tf.convert_to_tensor(target))\r\n","        raise StopIteration()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWe-1dvdKrFo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610729788310,"user_tz":-60,"elapsed":1564,"user":{"displayName":"Jan Kettler","photoUrl":"","userId":"11578283291043874177"}},"outputId":"e7165f88-439d-465b-95f2-d9a690072259"},"source":["\r\n","ds = tf.data.Dataset.from_generator(generator=gen_class, output_shapes=([25,3],None) ,output_types=(tf.dtypes.float32, tf.dtypes.float32), args= [25])\r\n","ds = (ds.cache().shuffle(buffer_size = 10000, reshuffle_each_iteration=True).batch(64, )).prefetch(tf.data.experimental.AUTOTUNE)\r\n","print(ds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: ((None, 25, 3), <unknown>), types: (tf.float32, tf.float32)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wjWwWGXud8U6"},"source":["##LSTM implementation"]},{"cell_type":"markdown","metadata":{"id":"Z1TUSReKMO2V"},"source":["###LSTM_Cell"]},{"cell_type":"code","metadata":{"id":"b9k94yked-TL"},"source":["class LSTM_Cell(tf.keras.Model):\r\n","    def __init__(self):\r\n","        super(LSTM_Cell, self).__init__()\r\n","        self.input_units = 3\r\n","        self.cell_state = tf.zeros([64,self.input_units],tf.dtypes.float32)\r\n","        self.forget_gate = tf.keras.layers.Dense(self.input_units, activation = \"sigmoid\", bias_initializer= \"ones\")\r\n","        self.input_gate = tf.keras.layers.Dense(self.input_units, activation = \"sigmoid\")\r\n","        self.cell_state_candidates = tf.keras.layers.Dense(self.input_units, activation = \"tanh\")\r\n","        self.output_gate = tf.keras.layers.Dense(self.input_units, activation = \"sigmoid\")\r\n","\r\n","    #@tf.function    \r\n","    def __call__(self,input, hs, Training = True):\r\n","        # concatenate the input of our current timestep with our current hidden state\r\n","        concat_input = tf.concat([input, hs], axis = -1)\r\n","\r\n","        # calculate the forget, input and output filter aswell as our new cell-state-candidates based on the concatenated input\r\n","        forget_filter = self.forget_gate(concat_input)\r\n","        input_filter = self.input_gate(concat_input)\r\n","        candidates = self.cell_state_candidates(concat_input)\r\n","        output_filter = self.output_gate(concat_input)\r\n","\r\n","        #calculate new cellstate from our filters and old cellstate\r\n","        \r\n","        self.cell_state = forget_filter * self.cell_state + input_filter* candidates\r\n","        \r\n","        #calculate new hidden-state/output based on our new cellstate and output_filter\r\n","                #(1,1)              (1,4)                                 (4,1)\r\n","        hs = output_filter * tf.keras.activations.tanh(self.cell_state)\r\n","        return tf.convert_to_tensor(hs)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jC436Z-MSvA"},"source":["###LSTM"]},{"cell_type":"code","metadata":{"id":"AReZuYCRMWAN"},"source":["class LSTM(tf.keras.Model):\r\n","\r\n","  def __init__(self):\r\n","\r\n","    super(LSTM, self).__init__()\r\n","    self.input_units = 3\r\n","\r\n","    self.input_layer = tf.keras.layers.Dense(30, activation = \"sigmoid\")\r\n","    self.lstm_layer = LSTM_Cell()\r\n","    self.output_layer = tf.keras.layers.Dense(3, activation = \"softmax\")\r\n","\r\n","  #@tf.function\r\n","  def call(self, input, hs, training = True):\r\n","\r\n","    input = self.input_layer(input)\r\n","    for idx in tf.range(input.shape[1]):\r\n","      hs = self.lstm_layer(input[:,idx,:],hs)\r\n","    hs = self.output_layer(hs)\r\n","    return hs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S0FBTJ35mjXy"},"source":["##Training the Model"]},{"cell_type":"code","metadata":{"id":"jvVj3xTFEAeW"},"source":["#@tf.function\r\n","def train_step(model, input, target, loss_function, optimizer, training = True):\r\n","    with tf.GradientTape() as tape:\r\n","        hs = tf.zeros([input.shape[0],input.shape[2]])\r\n","        expectation = model(input, hs, training)        \r\n","        loss = loss_function(target, expectation)\r\n","        gradients = tape.gradient(loss, model.trainable_variables)\r\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n","    train_accuracy = tf.cast(tf.argmax(target, axis = 1) == tf.argmax(expectation, axis = 1), tf.int16)\r\n","    return loss, train_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgiH3lGXW0Rv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610730541192,"user_tz":-60,"elapsed":216229,"user":{"displayName":"Jan Kettler","photoUrl":"","userId":"11578283291043874177"}},"outputId":"ec89dd31-7186-4e1c-a492-37adc36764f4"},"source":["num_epochs = 25\r\n","learning_rate = 0.0005\r\n","running_average_factor = 0.95\r\n","# use categorical crossentropy as loss function\r\n","loss_function = tf.keras.losses.CategoricalCrossentropy()\r\n","# use adam as optimizer\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n","# initialize the model\r\n","model = LSTM()\r\n","\r\n","# lists for plotting later\r\n","train_loss_list = []\r\n","train_accuracy_list = []\r\n","\r\n","# we train for the predefined number of epochs\r\n","for step in range(num_epochs):\r\n","    # reset average for each step\r\n","    changing_average_loss = 0\r\n","    # train the model with the training data\r\n","    for (input, target) in ds.take(100):\r\n","        # perform training step and store the loss and accuracy of step\r\n","        train_loss, train_accuracy = train_step(model, input, target, loss_function, optimizer)\r\n","        changing_average_loss = running_average_factor * changing_average_loss + (1 - running_average_factor) * train_loss\r\n","    train_loss_list.append(changing_average_loss.numpy())\r\n","    train_accuracy_list.append(np.mean(train_accuracy))\r\n","    \r\n","    # print values during training\r\n","    print(\"Epoch: \" + str(step + 1))\r\n","    print(\"Training Accuracy: \" + str(train_accuracy_list[step]))\r\n","    print(\"Train Loss: \" + str(train_loss_list[step]))\r\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1\n","Training Accuracy: 0.34375\n","Train Loss: 1.1295981\n","\n","Epoch: 2\n","Training Accuracy: 0.4375\n","Train Loss: 1.009766\n","\n","Epoch: 3\n","Training Accuracy: 0.390625\n","Train Loss: 0.9927715\n","\n","Epoch: 4\n","Training Accuracy: 0.4375\n","Train Loss: 0.98774403\n","\n","Epoch: 5\n","Training Accuracy: 0.4375\n","Train Loss: 0.99136287\n","\n","Epoch: 6\n","Training Accuracy: 0.484375\n","Train Loss: 0.9964152\n","\n","Epoch: 7\n","Training Accuracy: 0.484375\n","Train Loss: 0.993147\n","\n","Epoch: 8\n","Training Accuracy: 0.40625\n","Train Loss: 0.99060357\n","\n","Epoch: 9\n","Training Accuracy: 0.40625\n","Train Loss: 0.99102557\n","\n","Epoch: 10\n","Training Accuracy: 0.578125\n","Train Loss: 0.9922195\n","\n","Epoch: 11\n","Training Accuracy: 0.40625\n","Train Loss: 0.998401\n","\n","Epoch: 12\n","Training Accuracy: 0.484375\n","Train Loss: 0.9935434\n","\n","Epoch: 13\n","Training Accuracy: 0.484375\n","Train Loss: 0.9929112\n","\n","Epoch: 14\n","Training Accuracy: 0.515625\n","Train Loss: 0.9975654\n","\n","Epoch: 15\n","Training Accuracy: 0.5\n","Train Loss: 0.9939903\n","\n","Epoch: 16\n","Training Accuracy: 0.4375\n","Train Loss: 0.99962914\n","\n","Epoch: 17\n","Training Accuracy: 0.453125\n","Train Loss: 0.9908057\n","\n","Epoch: 18\n","Training Accuracy: 0.375\n","Train Loss: 0.9929808\n","\n","Epoch: 19\n","Training Accuracy: 0.546875\n","Train Loss: 0.9946979\n","\n","Epoch: 20\n","Training Accuracy: 0.515625\n","Train Loss: 0.98894966\n","\n","Epoch: 21\n","Training Accuracy: 0.53125\n","Train Loss: 0.9915759\n","\n","Epoch: 22\n","Training Accuracy: 0.46875\n","Train Loss: 0.9919509\n","\n","Epoch: 23\n","Training Accuracy: 0.484375\n","Train Loss: 1.0011429\n","\n","Epoch: 24\n","Training Accuracy: 0.328125\n","Train Loss: 0.99449605\n","\n","Epoch: 25\n","Training Accuracy: 0.5\n","Train Loss: 0.9875047\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zDjp3MZVB2OU"},"source":["##Training-task"]},{"cell_type":"markdown","metadata":{"id":"b6NBHjIZB9YK"},"source":["###truncated BPTT\r\n","\r\n","- No we should probably not use t-BPTT since our input-sequences always come in the same sizes, so we do not need to normalize them.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"N2WWU9wwDu5k"},"source":["###Regression or Classification\r\n","\r\n","- We should take this task as a classification task since we just want to assign three possible lables: \r\n","1. The first shown context-number appears most often.\r\n","2. The second shwon context-number appears most often.\r\n","3. Both numbers appear the same amount of time.\r\n","Also getting typical regression predicitons do not make sense for this task and do not allow us to learn from them."]}]}